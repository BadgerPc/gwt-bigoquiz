<?xml version="1.0" encoding="UTF-8"?>
<quiz xmlns="http://bigoquiz.com/document" format_version="1" id="algorithms_analysis">
    <title>Algorithms Analysis</title>

    <section id="master-theorem">
        <title>Master Theorem</title>

        <subsection id="master-theorem-using" answers_as_choices="true">
            <title>Using: T(n) = a T(n / b) + f(n)</title>
            <link>https://en.wikipedia.org/wiki/Master_theorem</link>
            <question id="parts-less-than">
                <text>If f(n) ∈ O(n ^ c) where c &lt; logb(a)</text>
                <answer>Case 1: T(n) ∈ Θ(n ^ logb(a))</answer>
            </question>
            <question id="parts-equal">
                <text>If f(n) ∈ Θ(n ^ c ⋅ (log(n))^k where c = logb(a)</text>
                <answer>Case 2: T(n) ∈ Θ(n ^ c ⋅ (log(n))^(k+1))</answer>
            </question>
            <question id="parts-greater-than">
                <text>If f(n) ∈ Ω(n ^ c) where c &gt; logb(a)</text>
                <answer>Case 3: T(n) ∈ Θ(f(n))</answer>
            </question>
        </subsection>

        <!-- See end of Erik Demaine's MIT lecture: http://videolectures.net/mit6046jf05_demaine_lec02/
        and https://www.quora.com/What-is-an-intuitive-explanation-of-the-Master-Theorem/answer/Brian-Bi -->
        <subsection id="master-theorem-intuition" answers_as_choices="true">
            <title>Intuition: T(n) = a T(n / b) + f(n), c &lt;/=/&gt; logb(a)</title>
            <link>https://en.wikipedia.org/wiki/Master_theorem</link>
            <question id="parts-less-than">
                <text>Case 1: T(n) ∈ Θ(n ^ logb(a))</text>
                <answer>Cost increases geometrically. Recursive work dominates. The bottom-most level has a constant fraction of the cost.</answer>
            </question>
            <question id="parts-equal">
                <text>Case 2: T(n) ∈ Θ(n ^ c ⋅ (log(n))^(k+1))</text>
                <answer>Cost is approximately the same on each level. Total cost is number of levels times first level's cost.</answer>
            </question>
            <question id="parts-greater-than">
                <text>Case 3: T(n) ∈ Θ(f(n))</text>
                <answer>Cost decreases geometrically. Non-recursive work dominates. The top-most level has a constant fraction of the work.</answer>
            </question>
        </subsection>

        <subsection id="master-theorem-parts" answers_as_choices="true">
            <title>Parts: T(n) &lt;= a T(n / b) + f(n)</title>
            <link>https://en.wikipedia.org/wiki/Master_theorem</link>
            <question id="parts-a">
                <text>a</text>
                <answer>Number of recursive calls</answer>
            </question>
            <question id="parts-b">
                <text>b</text>
                <answer>Input size shrinkage factor</answer>
            </question>
            <question id="parts-d">
                <text>f(n)</text>
                <answer>Cost of non-recursive work (divide and combine)</answer>
            </question>
        </subsection>
    </section>

    <section id="simplified-master-theorem">
        <title>Simplified Master Theorem</title>

        <!-- This is based on Tim Roughgarden's description of the master method,
        apparently based on the description in Sanjoy Dasgupta's "Algorithms" textbook.
        Note that it lists the cases in a different order, with = first. -->
        <subsection id="simplified-master-theorem-using" answers_as_choices="true">
            <title>Using: T(n) = a T(n / b) + f(n)</title>
            <link>https://en.wikipedia.org/wiki/Master_theorem</link>
            <question id="simplified-parts-equal">
                <text>a == bᵈ</text>
                <answer>O(nᵈ log(n))</answer>
            </question>
            <question id="simplified-parts-less-than">
                <text>a &lt; bᵈ</text>
                <answer>O(nᵈ)</answer>
            </question>
            <question id="simplified-parts-greater-than">
                <text>a &gt; bᵈ</text>
                <answer>O(n ^ logb(a))</answer>
            </question>
        </subsection>

        <subsection id="simplified-master-theorem-parts" answers_as_choices="true">
            <title>Parts: T(n) &lt;= a T(n / b) + O(nᵈ)</title>
            <link>https://en.wikipedia.org/wiki/Master_theorem</link>
            <question id="simplified-parts-a">
                <text>a</text>
                <answer>Number of recursive calls</answer>
            </question>
            <question id="simplified-parts-b">
                <text>b</text>
                <answer>Input size shrinkage factor</answer>
            </question>
            <question id="simplified-parts-d">
                <text>d</text>
                <answer>Exponent in running time of non-recursive work (divide and combine)</answer>
            </question>
        </subsection>
    </section>

    <!-- TODO: I am not satisfied with these explanations. murrayc -->
    <section id="amortized-analysis" answers_as_choices="true">
        <title>Amortized Analysis</title>
        <link>https://en.wikipedia.org/wiki/Amortized_analysis</link>
        <question id="amortized-analysis-aggregate-analysis">
            <text>Aggregate Analysis</text>
            <link>https://en.wikipedia.org/wiki/Aggregate_analysis</link>
            <answer>Determines an average worst case time for each operation based on the worst case time for n operations of any type.</answer>
        </question>
        <question id="amortized-analysis-accounting-method">
            <text>Accounting Method</text>
            <link>https://en.wikipedia.org/wiki/Accounting_method</link>
            <answer>Assigns a cost for each operation that includes the cost of affected future operations. For instance, the cost of each insert would include the cost of each subsequent delete.</answer>
        </question>
        <question id="amortized-analysis-potential-method">
            <text>Potential Method</text>
            <link>https://en.wikipedia.org/wiki/Potential_method</link>
            <answer>Chooses a potential function that associates a potential with each state of the data structure. The amortized cost of each operation is the actual cost plus the change in potential.</answer>
        </question>
    </section>

    <!-- TODO: Greedy algorithms: Proof by contradiction.
         TODO: Greedy algorithms: Proof by contradiction: Assume that the subproblem solution is not optimal (not part of the whole solution).
         -->
    <!-- TODO: Other proofs by. -->

    <!-- Time complexity of recursive algorithms:
      Factorial: O(n): Just reduces n by 1 each time, doing same amount of work each time. n * k -> O(n).
      Fibonacci (naive): Increases work by 2 each time: O(phi ^ n).
    -->

    <!-- Space complexity of recursive algorithms: (TODO: Just the max depth?)
     Factorial: O(n): Each call results in 1 recursive call, until they all unwind.
     Fibonacci (naive): Increases work by 2 each time: O(phi ^ n).
   -->

    <!-- TODO: Amortized Analysis -->

</quiz>