<?xml version="1.0" encoding="UTF-8"?>
<quiz xmlns="http://bigoquiz.com/document" format_version="1" id="algorithms">
    <title>Algorithms</title>
    <section id="algorithms-sort">
        <title>Sorting Algorithms</title>
            <subsection id="algorithms-sort-description" answers_as_choices="true" and_reverse="true">
                <title>Description</title>

                <question id="algorithms-sort-description-quicksort">
                    <text>Quicksort</text>
                    <link>https://en.wikipedia.org/wiki/Quicksort</link>
                    <answer>Find partition and recurse.</answer>
                </question>
                <question id="algorithms-sort-description-mergesort">
                    <text>Mergesort</text>
                    <link>https://en.wikipedia.org/wiki/Merge_sort</link>
                    <answer>Divide, sort, merge, recurse.</answer>
                </question>
                <question id="algorithms-sort-description-heapsort">
                    <text>Heapsort</text>
                    <link>https://en.wikipedia.org/wiki/Heapsort</link>
                    <answer>Like Selection Sort, but first heapifies the array and then chooses the largest values.</answer>
                </question>
                <question id="algorithms-sort-description-timsort">
                    <text>Timsort</text>
                    <link>https://en.wikipedia.org/wiki/Timsort</link>
                    <answer>Hybrid of Mergesort and Insertion Sort. Finds ordered subsequences.</answer>
                </question>
                <question id="algorithms-sort-description-insertion-sort">
                    <text>Insertion Sort</text>
                    <link>https://en.wikipedia.org/wiki/Insertion_sort</link>
                    <answer>Move each item, by neighbour swaps, to its location in the sorted list. Like sorting a hand of cards.</answer>
                </question>
                <question id="algorithms-sort-description-selection-sort">
                    <text>Selection Sort</text>
                    <link>https://en.wikipedia.org/wiki/Selection_sort</link>
                    <answer>Repeatedly finds smallest item and moves it to the end of the sorted section at the start.</answer>
                </question>
                <question id="algorithms-sort-description-shell-sort">
                    <text>Shellsort</text>
                    <link>https://en.wikipedia.org/wiki/Shellsort</link>
                    <answer>Like Insertion Sort, but considering every hth element, then again for lower h, until h is 1.</answer>
                </question>
                <question id="algorithms-sort-description-counting-sort">
                    <text>Counting Sort</text>
                    <answer>Count how often each value appears. Calculate the start index for each value. Copy each value to the start index in the output, decrementing the start index each time for each value.</answer>
                </question>
                <question id="algorithms-sort-description-radix-sort">
                    <text>Radix Sort</text>
                    <link>https://en.wikipedia.org/wiki/Radix_sort</link>
                    <answer>Sort d times, each time examining just b bits, using counting sort or bucket sort.</answer>
                </question>
                <question id="algorithms-sort-description-bucket-sort">
                    <text>Bucket Sort</text>
                    <answer>Scatter items into buckets for ranges. Sort each bucket. Fill the array from the sorted buckets.</answer>
                </question>
                <question id="algorithms-sort-description-bubble-sort">
                    <text>Bubble Sort</text>
                    <answer>Compare and swap pairs until no more swaps are necessary.</answer>
                </question>

                <!-- TODO? Burstsort (uses a Trie) https://en.wikipedia.org/wiki/Burstsort -->
            </subsection>

            <!-- This is a duplication of the more complete information in bigo.xml,
            but it seems useful to have it here too. -->
            <subsection id="algorithms-sort-time" answers_as_choices="true" and_reverse="true">
                <title>Average Time</title>

                <question id="algorithms-sort-time-quicksort">
                    <text>Quicksort</text>
                    <link>https://en.wikipedia.org/wiki/Quicksort</link>
                    <answer>O(n log(n))</answer>
                </question>
                <question id="algorithms-sort-time-mergesort">
                    <text>Mergesort</text>
                    <link>https://en.wikipedia.org/wiki/Merge_sort</link>
                    <answer>O(n log(n))</answer>
                </question>
                <question id="algorithms-sort-time-heapsort">
                    <text>Heapsort</text>
                    <link>https://en.wikipedia.org/wiki/Heapsort</link>
                    <answer>O(n log(n))</answer>
                </question>
                <question id="algorithms-sort-time-timsort">
                    <text>Timsort</text>
                    <link>https://en.wikipedia.org/wiki/Timsort</link>
                    <answer>O(n log(n))</answer>
                </question>
                <question id="algorithms-sort-time-insertion-sort">
                    <text>Insertion Sort</text>
                    <link>https://en.wikipedia.org/wiki/Insertion_sort</link>
                    <answer>O(n ^ 2)</answer>
                </question>
                <question id="algorithms-sort-time-selection-sort">
                    <text>Selection Sort</text>
                    <link>https://en.wikipedia.org/wiki/Selection_sort</link>
                    <answer>O(n ^ 2)</answer>
                </question>
                <question id="algorithms-sort-time-shell-sort">
                    <text>Shellsort</text>
                    <link>https://en.wikipedia.org/wiki/Shellsort</link>
                    <answer>O(n log(n)^2)</answer>
                </question>
                <question id="algorithms-sort-time-counting-sort">
                    <text>Counting Sort</text>
                    <answer>O(n + k)</answer>
                </question>
                <question id="algorithms-sort-time-radix-sort">
                    <text>Radix Sort</text>
                    <link>https://en.wikipedia.org/wiki/Radix_sort</link>
                    <answer>O(nk)</answer>
                </question>
                <question id="algorithms-sort-time-bucket-sort">
                    <text>Bucket Sort</text>
                    <answer>O(n + k)</answer>
                </question>
                <question id="algorithms-sort-time-bubble-sort">
                    <text>Bubble Sort</text>
                    <answer>O(n ^ 2)</answer>
                </question>

                <!-- TODO? Burstsort (uses a Trie) https://en.wikipedia.org/wiki/Burstsort -->
            </subsection>


        <subsection id="algorithms-sort-advantages" answers_as_choices="true">
            <title>Advantages</title>

            <question id="algorithms-sort-advantages-quicksort">
                <text>Quicksort</text>
                <link>https://en.wikipedia.org/wiki/Quicksort</link>
                <answer>In-place (O(log(n)) space). Data Locality. Small coefficient of O(n log(n)).</answer>
            </question>
            <question id="algorithms-sort-advantages-mergesort">
                <text>Mergesort</text>
                <link>https://en.wikipedia.org/wiki/Merge_sort</link>
                <answer>O(n log(n)) worst case time. Stable. Data Locality. Parallelizable. Can be External.</answer>
            </question>
            <question id="algorithms-sort-advantages-heapsort">
                <text>Heapsort</text>
                <link>https://en.wikipedia.org/wiki/Heapsort</link>
                <answer>O(n log(n)) worst case time. O(1) space.</answer>
            </question>
            <question id="algorithms-sort-advantages-timsort">
                <text>Timsort</text>
                <link>https://en.wikipedia.org/wiki/Timsort</link>
                <answer>O(n) best case time.</answer>
            </question>
            <question id="algorithms-sort-advantages-insertion-sort">
                <text>Insertion Sort</text>
                <answer>Fast for small n, even though it has O(n^2) average time. Fast for nearly-sorted items. Online algorithm.</answer>
                <!-- See other online sorts: https://en.wikipedia.org/wiki/Category:Online_sorts -->
            </question>
            <question id="algorithms-sort-advantages-selection-sort">
                <text>Selection Sort</text>
                <answer>Minimal swaps.</answer>
            </question>
            <!-- None?
            <question id="algorithms-sort-advantages-shell-sort">
                <text>Shell Sort</text>
                <answer></answer>
            </question>
            -->
            <question id="algorithms-sort-advantages-counting-sort">
                <text>Counting Sort</text>
                <link>https://en.wikipedia.org/wiki/Counting_sort</link>
                <answer>O(n) time, but with O(m) space</answer>
                <note>Therefore, this is unsuitable when the range m is much greater than n.</note>
            </question>
            <question id="algorithms-sort-advantages-radix-sort">
                <text>Radix Sort</text>
                <link>https://en.wikipedia.org/wiki/Radix_sort</link>
                <answer>O(n) time. Minimal space.</answer>
            </question>
            <question id="algorithms-sort-advantages-bucket-sort">
                <text>Bucket Sort</text>
                <link>https://en.wikipedia.org/wiki/Bucket_sort</link>
                <answer>Like Counting Sort, but uses O(n) space instead of O(m) space,</answer>
            </question>
            <!-- None?
            <question id="algorithms-sort-advantages-bubble-sort">
                <text>Bubble Sort</text>
                <answer></answer>
            </question>
            -->
        </subsection>

        <!-- TODO: What do they need space for? -->

        <subsection id="algorithms-sort-stability" answers_as_choices="true">
            <title>Sort Stability</title>
            <link>https://en.wikipedia.org/wiki/Stable_sort</link>

            <question id="algorithms-sort-stability-quicksort">
                <text>Quicksort</text>
                <link>https://en.wikipedia.org/wiki/Quicksort</link>
                <answer>Unstable</answer>
            </question>
            <question id="algorithms-sort-stability-mergesort">
                <text>Mergesort</text>
                <link>https://en.wikipedia.org/wiki/Merge_sort</link>
                <answer>Stable</answer>
            </question>
            <question id="algorithms-sort-stability-heapsort">
                <text>Heapsort</text>
                <link>https://en.wikipedia.org/wiki/Heapsort</link>
                <answer>Unstable</answer>
            </question>
            <question id="algorithms-sort-stability-timsort">
                <text>Timsort</text>
                <link>https://en.wikipedia.org/wiki/Timsort</link>
                <answer>Stable</answer>
            </question>
            <question id="algorithms-sort-stability-insertion-sort">
                <text>Insertion Sort</text>
                <link>https://en.wikipedia.org/wiki/Insertion_sort</link>
                <answer>Stable</answer>
            </question>
            <question id="algorithms-sort-stability-selection-sort">
                <text>Selection Sort</text>
                <link>https://en.wikipedia.org/wiki/Selection_sort</link>
                <answer>Unstable</answer>
            </question>
            <question id="algorithms-sort-stability-shellsort">
                <text>Shellsort</text>
                <link>https://en.wikipedia.org/wiki/Shellsort</link>
                <answer>Unstable</answer>
            </question>
            <question id="algorithms-sort-stability-counting-sort">
                <text>Counting Sort</text>
                <answer>Stable</answer>
            </question>
            <question id="algorithms-sort-stability-radix-sort">
                <text>Radix Sort</text>
                <link>https://en.wikipedia.org/wiki/Radix_sort</link>
                <answer>Stable</answer>
            </question>
            <question id="algorithms-sort-stability-bucket-sort">
                <text>Bucket Sort</text>
                <answer>Stable</answer>
            </question>
            <question id="algorithms-sort-stability-bubble-sort">
                <text>Bubble Sort</text>
                <answer>Stable</answer>
            </question>
        </subsection>

        <subsection id="algorithms-sort-inplace" answers_as_choices="true">
            <title>In Place or Not</title>
            <link>https://en.wikipedia.org/wiki/In-place_algorithm</link>

            <question id="algorithms-sort-inplace-quicksort">
                <text>Quicksort</text>
                <link>https://en.wikipedia.org/wiki/Quicksort</link>
                <answer>In-place (but small additional space needed)</answer>
            </question>
            <question id="algorithms-sort-inplace-mergesort">
                <text>Mergesort</text>
                <link>https://en.wikipedia.org/wiki/Merge_sort</link>
                <answer>Not in-place</answer>
            </question>
            <question id="algorithms-sort-inplace-heapsort">
                <text>Heapsort</text>
                <link>https://en.wikipedia.org/wiki/Heapsort</link>
                <answer>In-place</answer>
            </question>
            <question id="algorithms-sort-inplace-timsort">
                <text>Timsort</text>
                <link>https://en.wikipedia.org/wiki/Timsort</link>
                <answer>Not in-place</answer>
            </question>
            <question id="algorithms-sort-inplace-insertion-sort">
                <text>Insertion Sort</text>
                <link>https://en.wikipedia.org/wiki/Insertion_sort</link>
                <answer>In-place</answer>
            </question>
            <question id="algorithms-sort-inplace-selection-sort">
                <text>Selection Sort</text>
                <link>https://en.wikipedia.org/wiki/Selection_sort</link>
                <answer>In-place</answer>
            </question>
            <question id="algorithms-sort-inplace-shellsort">
                <text>Shellsort</text>
                <link>https://en.wikipedia.org/wiki/Shellsort</link>
                <answer>In-place</answer>
            </question>
            <question id="algorithms-sort-inplace-counting-sort">
                <text>Counting Sort</text>
                <answer>Not in-place</answer>
            </question>
            <question id="algorithms-sort-inplace-radix-sort">
                <text>Radix Sort</text>
                <link>https://en.wikipedia.org/wiki/Radix_sort</link>
                <answer>Not in-place</answer>
            </question>
            <question id="algorithms-sort-inplace-bucket-sort">
                <text>Bucket Sort</text>
                <answer>Not in-place</answer>
            </question>
            <question id="algorithms-sort-inplace-bubble-sort">
                <text>Bubble Sort</text>
                <answer>In-place</answer>
            </question>
        </subsection>

        <subsection id="algorithms-sort-comparison" answers_as_choices="true">
            <title>Comparison or Non-Comparison</title>
            <link>https://en.wikipedia.org/wiki/Comparison_sort</link>

            <question id="algorithms-sort-comparison-quicksort">
                <text>Quicksort</text>
                <link>https://en.wikipedia.org/wiki/Quicksort</link>
                <answer>Comparison</answer>
            </question>
            <question id="algorithms-sort-comparison-mergesort">
                <text>Mergesort</text>
                <link>https://en.wikipedia.org/wiki/Merge_sort</link>
                <answer>Comparison</answer>
            </question>
            <question id="algorithms-sort-comparison-heapsort">
                <text>Heapsort</text>
                <link>https://en.wikipedia.org/wiki/Heapsort</link>
                <answer>Comparison</answer>
            </question>
            <question id="algorithms-sort-comparison-timsort">
                <text>Timsort</text>
                <link>https://en.wikipedia.org/wiki/Timsort</link>
                <answer>Comparison</answer>
            </question>
            <question id="algorithms-sort-comparison-insertion-sort">
                <text>Insertion Sort</text>
                <link>https://en.wikipedia.org/wiki/Insertion_sort</link>
                <answer>Comparison</answer>
            </question>
            <question id="algorithms-sort-comparison-selection-sort">
                <text>Selection Sort</text>
                <link>https://en.wikipedia.org/wiki/Selection_sort</link>
                <answer>Comparison</answer>
            </question>
            <question id="algorithms-sort-comparison-shellsort">
                <text>Shellsort</text>
                <link>https://en.wikipedia.org/wiki/Shellsort</link>
                <answer>Comparison</answer>
            </question>
            <question id="algorithms-sort-comparison-counting-sort">
                <text>Counting Sort</text>
                <answer>Non-Comparison</answer>
            </question>
            <question id="algorithms-sort-comparison-radix-sort">
                <text>Radix Sort</text>
                <link>https://en.wikipedia.org/wiki/Radix_sort</link>
                <answer>Non-Comparison</answer>
            </question>
            <question id="algorithms-sort-comparison-bucket-sort">
                <text>Bucket Sort</text>
                <answer>Comparison</answer>
            </question>
            <question id="algorithms-sort-comparison-bubble-sort">
                <text>Bubble Sort</text>
                <answer>Comparison</answer>
            </question>
        </subsection>
    </section>

    <section id="algorithms-substring-search">
        <title>Substring Search Algorithms</title>
        <subsection id="algorithms-substring-search-description" answers_as_choices="true" and_reverse="true">
            <title>Description</title>
            <question id="algorithms-substring-search-description-kmp">
                <text>Knuth-Morris-Pratt (KMP)</text>
                <link>https://en.wikipedia.org/wiki/Knuth%E2%80%93Morris%E2%80%93Pratt_algorithm</link>
                <answer>Pre-calculate a Deterministic Finite Automata (DFA), (also known as &quot;partial match table&quot; or &quot;failure function&quot;) that tells us, for each mismatch, where in the pattern we would need to start comparing characters again. For instance, this avoids re-comparing a prefix of the pattern that is identical to a suffix of the part of the pattern compared so far.</answer>
            </question>

            <question id="algorithms-substring-search-description-boyer-moore">
                <text>Boyer-Moore</text>
                <link>https://en.wikipedia.org/wiki/Boyer%E2%80%93Moore_string_search_algorithm</link>
                <answer>Pre-calculate the right-most position of each character in the pattern. When there is a mismatch, skip ahead that much and compare again from the start of the pattern.</answer>
            </question>

            <question id="algorithms-substring-search-description-rabin-karp">
                <text>Rabin-Karp</text>
                <link>https://en.wikipedia.org/wiki/Rabin%E2%80%93Karp_algorithm</link>
                <answer>Compare hashes of the substrings, generating the next hash based on the previous hash.</answer>
            </question>

            <!-- TODO: Boyer-Moore-Horspool -->
            <!-- TODO: Aho-Corasick -->
            <!-- TODO: Z Algorithm -->
        </subsection>

        <subsection id="algorithms-substring-search-time-complexity" answers_as_choices="true" and_reverse="true">
            <title>Time Complexity</title>
            <!-- These are based on page 779 of Sedgewicks's Algorithms book, 4th edition. -->
            <question id="algorithms-substring-search-time-kmp">
                <text>Knuth-Morris-Pratt (KMP)</text>
                <link>https://en.wikipedia.org/wiki/Knuth%E2%80%93Morris%E2%80%93Pratt_algorithm</link>
                <answer>O(2N), typically 1.1N</answer>
            </question>

            <question id="algorithms-substring-search-time-boyer-moore">
                <text>Boyer-Moore</text>
                <link>https://en.wikipedia.org/wiki/Boyer%E2%80%93Moore_string_search_algorithm</link>
                <answer>O(3N), typically N/M</answer>
            </question>

            <question id="algorithms-substring-search-time-rabin-karp">
                <text>Rabin-Karp</text>
                <link>https://en.wikipedia.org/wiki/Rabin%E2%80%93Karp_algorithm</link>
                <answer>O(7N), typically 7N</answer>
            </question>
        </subsection>

        <subsection id="algorithms-substring-search-extra-space" answers_as_choices="true" and_reverse="true">
            <title>Extra space</title>
            <!-- These are based on page 779 of Sedgewicks's Algorithms book, 4th edition. -->
            <question id="algorithms-substring-search-extra-space-kmp">
                <text>Knuth-Morris-Pratt (KMP)m</text>
                <link>https://en.wikipedia.org/wiki/Knuth%E2%80%93Morris%E2%80%93Pratt_algorithm</link>
                <answer>MR (substring length * alphabet size)</answer>
            </question>

            <question id="algorithms-substring-search-extra-space-boyer-moore">
                <text>Boyer-Moore</text>
                <link>https://en.wikipedia.org/wiki/Boyer%E2%80%93Moore_string_search_algorithm</link>
                <answer>R (alphabet size)</answer>
            </question>

            <question id="algorithms-substring-search-extra-space-rabin-karp">
                <text>Rabin-Karp</text>
                <link>https://en.wikipedia.org/wiki/Rabin%E2%80%93Karp_algorithm</link>
                <answer>1</answer>
            </question>
        </subsection>

        <subsection id="algorithms-substring-search-backtracking" answers_as_choices="true">
            <title>Backup in input?</title>
            <question id="algorithms-substring-search-backtracking-kmp">
                <text>Knuth-Morris-Pratt (KMP)m</text>
                <link>https://en.wikipedia.org/wiki/Knuth%E2%80%93Morris%E2%80%93Pratt_algorithm</link>
                <answer>No backup required</answer>
            </question>

            <question id="algorithms-substring-search-backtracking-boyer-moore">
                <text>Boyer-Moore</text>
                <link>https://en.wikipedia.org/wiki/Boyer%E2%80%93Moore_string_search_algorithm</link>
                <answer>Backup required</answer>
            </question>

            <question id="algorithms-substring-search-backtracking-rabin-karp">
                <text>Rabin-Karp</text>
                <link>https://en.wikipedia.org/wiki/Rabin%E2%80%93Karp_algorithm</link>
                <answer>No backup required</answer>
            </question>
        </subsection>
    </section>

    <!-- Greedy Algorithms: Dijkstra's, Scheduling, Make change (canonical coins).
       Huffman Codes: (Bottom-up. Merge items (or combined items) with least frequency, at each step, giving the subtree the sum of their frequencies. Repeat.
          Wrong answer: merge highest frequencies.
          Wrong answer: Always merge with the sub-tree so far. Instead merge subtrees only when they are the 2 smallest items.
       Scheduling jobs: Length and weight. Completion Time.  Minimise W*C. So sort by
       Scheduling to avoid conflicts: Choose earliest finish time first. -->

    <!-- Divide and Conquer Algorithms:
    https://en.wikipedia.org/wiki/Divide_and_conquer_algorithms
    Quicksort, Mergesort, Hashing?, Convex Hull (Divide & Conquer), Strassen Matrix Multiplication,
    median finding (quickselect, or (better) median of medians),
    polynomial multiplication, FFT  -->

    <!-- Dynamic Programming Algorithms:
      Interval/Job Scheduling with weights:
        opt(R) = max 1 <= i <= n (wi + opt(Rfinish(i)))
        O(n^2) but apparently O(nlog(n)) is possible.
        Add the requirement that not all jobs can be done on the same machine (or by same person), then it is NP-complete.
      Bellmann-Ford:
      Floyd Warshall,
      Make change (non-canonical coins),
      String Comparison.
      Building an optimal binary search tree: https://en.wikipedia.org/wiki/Optimal_binary_search_tree#Knuth.27s_dynamic_programming_algorithm
        (though Huffmann codes can be done with greedy algorithm.)
        DP Algorithm chooses roots of left and right subtrees of subproblems. O(n^3) or O(n^2) if we check only between the left and right roots:
          https://stackoverflow.com/questions/16987670/dynamic-programming-why-knuths-improvement-to-optimal-binary-search-tree-on2
      Independent Set of a Path Graph (in Stanford/Coursera course, part 2): https://www.coursera.org/learn/algorithm-design-analysis-2/lecture/TZgJM/wis-in-path-graphs-a-reconstruction-algorithm
        1 Dimension. (O(n)):
          A[i] = max(A[i-1], A[i-2] + w)
      Independent Set of a Tree Graph (in Stanford/Coursera course, part 2, quiz 3)
        "Recall our dynamic programming algorithm for computing the maximum-weight independent set of a path graph. Consider the following proposed extension to more general graphs. Consider an undirected graph with positive vertex weights. For a vertex v, obtain the graph G′(v) by deleting v and its incident edges from G, and obtain the graph G″(v) from G by deleting v, its neighbors, and all of the corresponding incident edges from G. Let OPT(H) denote the value of a maximum-weight independent set of a graph H. Consider the formula OPT(G)=max{OPT(G′(v)),wv+OPT(G″(v))}, where v is an arbitrary vertex of G of weight wv"
      Sequence Alignment for Needleman-Wunsch score: https://www.coursera.org/learn/algorithm-design-analysis-2/lecture/QJkyp/optimal-substructure
        Score, with cost for gaps in either string. 3 cases: No gap, gap in X, or gap in Y. 2 Dimensions.
          A[i, j] = min(A[i-1, j-1], A[i-1, j] + gap_cost, A[i, j-1] + gap_cost)

      Minimum Spanning Tree: Useful for identifying clusters.
       -->

    <!-- Convex hull
    Median Finding -->


    <!-- Strings and sets:
      Set Cover:
      Set Packing:
      String Matching:
      Approximate string matching:
        DP with costs for substitution, insertion, and deletion.
          Longest Common subsequence (LCS) is string distance DP algorithm without substitution.
          Longest increasing subsequence?
        Bit-parallel algorithm (page 633 of TADM).
      Longest Common Subsequence:
        Difference to Longest Common Substring
        With Suffix Tree: Insert both strings into the same suffix tree. Find the deepest node that is for both strings. O(n + m).
        The DP algorithm is O(nm).
      Longest Repeated Substring:
        With Suffix Tree: Put the string in a suffix tree. Find the deepest internal node.
      Shortest Common Substring

      Find lexographic location of Tk in T1 to Tn: Put them all in a trie, with terminating $ on each string.

      Multiple ($-terminated) strings in a trie, with ordered edges.
        Lexographic order: in-order traversal gives us lexographic order.
        Lexographic predecessor: Find node that matches as much as possible. Predecessor is max in left sub-tree.
          (Maintain sub-tree min/max to make this faster.)
        Lexographic successor: Find node that matches as much as possible. Successor is min in right sub-tree.
          (Maintain sub-tree min/max to make this faster.)
        LCA, LCP.

      ( https://www.youtube.com/watch?v=NinWEPPrkDQ )
      Tray: (Used for Trie/Suffix-Tree nodes to give O(P + lg(sigma)) lookup with O(T) space.

      ( https://youtu.be/NinWEPPrkDQ?t=3683 )
      "document retrieval" data structure: To find the matching documents without finding all the individual matches,
      in O(P + #-of-documents-matching).
        Uses RMQ.





      String matching without pre-processing (for instance, without suffix tree):
      (Without string processing it's generally O(T) time (T=length of searched text),
       but with preprocessing it can be O(P) time (P=length of pattern), often in O(T) space.)
      Knuth-Morris-Pratt Algorithm
        https://en.wikipedia.org/wiki/Knuth%E2%80%93Morris%E2%80%93Pratt_algorithm
        "using precomputation to examine each text character only once"
      Boyer-Moore Algorithm
        https://en.wikipedia.org/wiki/Boyer%E2%80%93Moore_string_search_algorithm
        "Skips forward as many as possible for the search to succeed,"
      Rabin-Karp Algorithm
        https://en.wikipedia.org/wiki/Rabin%E2%80%93Karp_algorithm
        Uses a rolling hash.
        Uses: Detect plagiarism. Multiple pattern search.
          Wikipedia says "Inferior for single pattern searching to Knuth–Morris–Pratt algorithm, Boyer–Moore string search algorithm"
      Aho-Corasick Algorithm
        https://en.wikipedia.org/wiki/Aho%E2%80%93Corasick_algorithm
        "constructs a finite state machine that resembles a trie with additional links between the various internal nodes"

      https://www.youtube.com/watch?v=0rCFkuQS968
      Range Minimum Query (RMQ): Discover index of minimum value in a range of values:
        Put the values in a cartesian tree by splitting at the smallest values, recursively. Then the LCA of any two nodes is the RMQ.
        (You would need to keep an array associating each index with a tree node.)
        Can construct cartesian tree in (amortized) O(n) time by adding each node and, if necessary, walking up and splitting, making children into left nodes.
        The RMQ of indices in the the array also tells you the LCA of nodes in the tree.
        Use Euler Tour (in order traversal) to get RMQ array whose values only differ by 1.

      Level Ancestor (LA): Find kth parent of node in less than O(n) time.
       Long-path decomposition:

       Ladder decomposition:
    -->

    <!-- Postfix notation evaluation: Push operands on to stack. When we see an operator, we pop 2 items, apply, and push back.
         (We don't need to care about operator precedence. -->
    <!-- Proofs:
    Job/Interval Scheduling: Exchange argument: Order edge in order of duration / finish time.
    Kruskal's MST: Exchange Argument: Order edges in order of increasing cost. TODO-->

    <!-- Ukkonen's algorithm for Suffix Tree Construction:
         https://en.wikipedia.org/wiki/Ukkonen%27s_algorithm
         an "online algorithm" -->

    <!-- Online algorithms (Processes input piece by piece without having the whole input from the start.)
    https://en.wikipedia.org/wiki/Online_algorithm
      Insertion Sort
      Ukkonen's algorithm for Suffix Tree Construction
      Farach's algorithm for Suffix Tree Construction
      Insertion Sort
      Boyer-Moore?

    Offline algorithms: Need whole data from the start:
      Selection Sort
      McCreights's algorithm for Suffix Tree Construction

    -->

    <!-- Algorithms on images:
    2-Pass Connected Component Labelling to count distinct objects in image:
      https://en.wikipedia.org/wiki/Connected-component_labeling#Two-pass
      First pass:
      Examine the pixels left to right, in each row. Check neighbour pixels at left, above-left, above, and above right.
      If no neighbours are labelled, start a new label and start a new set in the UnionFind of labels.
      If a neighbour is labelled, but the pixel is not yet labelled, label the pixel with the neighbour's label.
      If a neighbour has a higher label than the pixel, join the pixel's label  and the neighbour label in the UnionFind.
      Second pass:
      Count the distinct roots in the UnionFind of labels.
    -->

    <!-- subset sum -->

</quiz>