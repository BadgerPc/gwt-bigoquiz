<?xml version="1.0" encoding="UTF-8"?>
<quiz xmlns="http://bigoquiz.com/document" format_version="1" id="system_architecture">
    <title>System Architecture and SRE (Site Reliability Engineering)</title>
    <link>https://en.wikipedia.org/wiki/Site_reliability_engineering</link>

    <section id="theory" answers_as_choices="true" and_reverse="true">
        <title>Theory</title>
        <!-- Much of this is inspired by http://bravenewgeek.com/from-the-ground-up-reasoning-about-distributed-systems-in-the-real-world/ -->

        <question id="theory-cap">
            <text>CAP Theorem (Brewer)</text>
            <link>https://en.wikipedia.org/wiki/CAP_theorem</link>
            <answer>It is impossible for a distributed computer system to provide more than 2 of these guarantees: Consistency, Availability, and Partition Tolerance.</answer>
        </question>

        <question id="theory-flp-result">
            <text>FLP Result (Fischer, Lynch, and Patterson)</text>
            <link>https://en.wikipedia.org/wiki/CAP_theorem</link>
            <answer>Impossibility of distributed consensus with one faulty process.</answer>
        </question>

        <question id="theory-two-generals-problem">
            <text>Two Generals Problem</text>
            <link>https://en.wikipedia.org/wiki/Two_Generals'_Problem</link>
            <answer>Coordinating an action between two actors via an unreliable link. Proved unsolvable.</answer>
            <note>Mitigated by accepting the unreliability of the link, for instance by sending duplicate messages.</note>
        </question>

        <question id="theory-byzantine-generals-problem">
            <text>Byzantine Generals Problem (Lamport)</text>
            <link>https://en.wikipedia.org/wiki/Byzantine_fault_tolerance#Byzantine_Generals.27_Problem</link>
            <answer>Coordinating an action via an unreliable link when the other actors may be unreliable or even traitorous.</answer>
            <note>Byzantine fault tolerance is possible if the loyal (non-faulty) generals agree unanimously.</note>
            <!-- When is this unsolvable? Wikipedia says "unsolvable in the face of arbitrary communication failures" though only on the 2 generals problem page: https://en.wikipedia.org/wiki/Two_Generals%27_Problem -->
        </question>

        <!-- This is more of a generic concurrency problem (deadlock) than a distributed-computing problem:
        <question id="theory-dining-philosophers-problem">
            <text>Dining Philosopher's Problem (Dijkstra)</text>
            <link>https://en.wikipedia.org/wiki/Dining_philosophers_problem</link>
            <answer>TODO</answer>
        </question>
        -->
    </section>

    <section id="golden-signals" answers_as_choices="true" and_reverse="true">
        <title>Four Golden Signals</title>

        <question id="golden-signals-latency">
            <text>Latency</text>
            <link>https://en.wikipedia.org/wiki/Latency_(engineering)</link>
            <answer>Time needed to service a request.</answer>
            <note>Different requests to the same service may have different latencies.</note>
        </question>

        <question id="golden-signals-traffic">
            <text>Traffic (throughout)</text>
            <link>https://en.wikipedia.org/wiki/Throughput</link>
            <answer>Rate of requests.</answer>
        </question>

        <question id="golden-signals-errors">
            <text>Errors</text>
            <answer>Rate of failing requests.</answer>
        </question>

        <question id="golden-signals-saturation">
            <text>Saturation</text>
            <answer>How fully a system's resources are used.</answer>
        </question>
    </section>

    <section id="hierarchy-of-reliability" answers_as_choices="true">
        <title>Mikey Dickerson's Hierarchy of Reliability</title>
        <link>https://docs.google.com/drawings/d/1kshrK2RLkW-XV8enmWZxeRFRgADj6d4Ru_w5txz_k9I/edit</link>
        <!-- TODO: Find a better link -->

        <question id="hierarchy-of-reliability-order">
            <text>Most fundamental first</text>
            <answer>Monitoring, Incident Response, Postmortem/Root Cause Analysis, Testing &amp; Release Procedures, Capacity Planning, Development, Product</answer>
        </question>

        <question id="hierarchy-of-reliability-reverse-order">
            <text>Most dependent first</text>
            <answer>Product, Capacity Planning, Development, Testing &amp; Release Procedures, Postmortem/Root Cause Analysis, Incident Response, Monitoring</answer>
        </question>
    </section>

    <section id="other-terminology" answers_as_choices="true">
        <title>Other Terminology</title>

        <question id="terminology-slo">
            <text>SLO</text>
            <answer>Service Level Objective</answer>
            <!-- TODO: How is it measured, for instance? -->
        </question>

        <question id="terminology-sla">
            <text>SLA</text>
            <answer>Service Level Agreement</answer>
        </question>

        <question id="terminology-failure-modes">
            <text>Failure Modes</text>
            <answer>The various things that can go wrong and how the overall service will behave.</answer>
            <note>For instance, when a machine dies, when a rack fails, when a cluster fails, when a datacenter fails, when a network fails between datacenters.</note>
        </question>
        <!-- TODO: Single point of failure. Unavailable dependencies. Request Deadlines. Deadline Propagation -->

        <question id="terminology-load-balancing">
            <text>Load Balancing</text>
            <link>https://en.wikipedia.org/wiki/Load_balancing_(computing)</link>
            <answer>Distributes incoming network traffic across several backend servers.</answer>
            <note>For instance, using DNS, or at the virtual IP address.</note>
        </question>

        <question id="terminology-load-shedding">
            <text>Load Shedding</text>
            <link>https://en.wikipedia.org/wiki/Load_Shedding</link>
            <answer>Ignore some requests to reduce load on a server so it may serve at least some requests.</answer>
        </question>

        <question id="terminology-graceful-degradation">
            <text>Graceful Degradation</text>
            <link>https://en.wikipedia.org/w/index.php?title=Graceful_degradation</link>
            <answer>Reduce the amount of work to be performed for each request.</answer>
        </question>

        <question id="terminology-rate-limiting">
            <text>Rate Limiting</text>
            <link>https://en.wikipedia.org/wiki/Rate_limiting</link>
            <answer>Controls the rate of traffic.</answer>
            <note>Can happen at reverse proxies or at the load balancers.</note>
        </question>

        <question id="terminology-sharding">
            <text>Sharding</text>
            <answer>Splitting databases across servers.</answer>
        </question>

        <question id="terminology-rollback">
            <text>Rollback</text>
            <link>https://en.wikipedia.org/wiki/Rollback_(data_management)</link>
            <answer>Returning data or software to previous states or versions.</answer>
        </question>

        <question id="terminology-exponential-backoff">
            <text>Exponential Backoff</text>
            <link>https://en.wikipedia.org/wiki/Exponential_backoff</link>
            <answer>Clients delay retries by increasing (by multiplication) intervals.</answer>
            <note>See also Jitter</note>
        </question>

        <question id="terminology-jitter">
            <text>Jitter</text>
            <link>https://www.awsarchitectureblog.com/2015/03/backoff.html</link>
            <answer>Adds randomness to retry delays to avoid clustering of requests.</answer>
            <note>See also Exponential Backoff</note>
        </question>

        <question id="terminology-vertical-scaling">
            <text>Vertical Scaling</text>
            <link>https://en.wikipedia.org/wiki/Scalability#Horizontal_and_vertical_scaling</link>
            <answer>Add resources (CPU, RAM, disk) to existing servers.</answer>
        </question>

        <question id="terminology-horizontal-scaling">
            <text>Horizontal Scaling</text>
            <link>https://en.wikipedia.org/wiki/Scalability#Horizontal_and_vertical_scaling</link>
            <answer>Add servers.</answer>
        </question>

        <question id="terminology-cascading-failure">
            <text>Cascading Failure</text>
            <link>https://en.wikipedia.org/wiki/Cascading_failure</link>
            <answer>When a failure triggers failures of successive parts of a system.</answer>
        </question>

        <question id="terminology-consistent-hashing">
            <text>Consistent Hashing</text>
            <link>https://en.wikipedia.org/wiki/Consistent_hashing</link>
            <answer>Allows sharding across servers, without needing rehashing and resharding when adding or removing a server.</answer>
            <video_url>https://youtu.be/PQY2_QopWTM</video_url>
            <!-- Also good: https://www.youtube.com/watch?v=jznJKL0CrxM -->
            <note>Keys and their values are stored in whatever node's hashed ID is a successor to the hash of the key. The nodes are considered to be in a ring, so there is always a successor. Keys are stored in multiple nodes, either by using successor nodes or by using multiple hash functions, so we don't lose data when a node fails. Each node can identify the node responsible for a key, for instance with a finger table.</note>
        </question>

        <!-- TODO?
        <question id="terminology-finger-tables">
            <text>Finger Tables</text>
            <link>https://en.wikipedia.org/wiki/Chord_(peer-to-peer)#Finger_table</link>
            <answer>TODO: O(log n) hops.</answer>
            <video_url>https://www.youtube.com/watch?v=GOOXa2GkPws</video_url>
        </question>
        -->

        <question id="terminology-eventual-consistency">
            <text>Eventual Consistency</text>
            <link>https://en.wikipedia.org/wiki/Eventual_consistency</link>
            <answer>Offers high availability by only ensuring that reads will all eventually return the latest value (converge), instead of guaranteeing consistency at all times.</answer>
            <note>This requires more conflict resolution.</note>
        </question>

        <question id="terminology-read-replication">
            <text>Read Replication</text>
            <answer>Writes are propagated to multiple replicas which serve only reads.</answer>
            <video_url>https://youtu.be/MbqAH5OzLjE</video_url>
        </question>

        <question id="terminology-consistent-scatter-gather">
            <text>Scatter/Gather</text>
            <answer>Request data from multiple servers, aggregate them on one server, and return an overall response.</answer>
        </question>

        <question id="terminology-map-reduce">
            <text>MapReduce</text>
            <link>https://en.wikipedia.org/wiki/MapReduce</link>
            <answer>Distributed batch processing of key/value pairs.</answer>
            <note>A Mapper splits key/value pairs into multiple key/value pairs, possibly outputting keys based on parts of the input value. The framework shuffles these keys, sorting and grouping equal keys together, and the Reducer then performs some aggregate calculation on these groups of key/value pairs.</note>
            <video_url>https://youtu.be/9CrWSHzy3BQ</video_url>
        </question>

        <question id="terminology-vector-clock">
            <text>Vector Clock</text>
            <link>https://en.wikipedia.org/wiki/Vector_clock</link>
            <answer>Discovers partial ordering of events and detects conflicts.</answer>
            <note>The vector of service IDs and associated sequence numbers is sent with every message. For each received message or sent message, the service increments its own sequence number, and uses the latest sequence number for other services. There is only a conflict when one message's vector clock is not contained within another message's vector clock. The application must resolve conflicts.</note>
            <video_url>https://youtu.be/IT5AqUEGLwM</video_url>
        </question>

        <question id="terminology-lamport-timestamp">
            <text>Lamport Timestamp</text>
            <link>https://en.wikipedia.org/wiki/Lamport_timestamps</link>
            <answer>Discovers partial ordering of events and detect conflicts (simple ordering).</answer>
            <note>The timestamp is sent with every message. For each sent message, the service adds 1 to its timestamp. For every received message, it adds 1 to the max of its own timestamp and the received timestamp. However, this can suggest conflicts between causally-unrelated events - vector clocks let us detect actual conflicts.</note>
            <video_url>https://youtu.be/CMBjvCzDVkY</video_url>
        </question>

    </section>

    <!-- This is not really a Google SRE theme: -->
    <!-- The answer text here is directly from the Wikipedia article. -->
    <section id="cap" answers_as_choices="true">
        <title>CAP Theorem</title>
        <link>https://en.wikipedia.org/wiki/CAP_theorem</link>

        <question id="cap-consistency">
            <text>Consistency</text>
            <answer>Every read receives the most recent write or an error.</answer>
        </question>

        <question id="cap-availability">
            <text>Availability</text>
            <answer>Every request receives a response, without guarantee that it contains the most recent version of the information.</answer>
        </question>

        <question id="cap-partition-tolerance">
            <text>Partition Tolerance</text>
            <answer>The system continues to operate despite an arbitrary number of messages being dropped by the network between nodes.</answer>
        </question>
    </section>

    <!-- This is not really a Google SRE theme: -->
    <section id="acid" answers_as_choices="true">
        <title>ACID</title>
        <link>https://en.wikipedia.org/wiki/ACID</link>

        <question id="acid-atomicity">
            <text>Atomicity</text>
            <link>https://en.wikipedia.org/wiki/ACID#Atomicity</link>
            <answer>Either all operations, or no operations, in a transaction succeed.</answer>
        </question>

        <question id="acid-consistency">
            <text>Consistency</text>
            <link>https://en.wikipedia.org/wiki/ACID#Consistency</link>
            <answer>Each transaction changes the state only to another valid state.</answer>
        </question>

        <question id="acid-isolation">
            <text>Isolation</text>
            <link>https://en.wikipedia.org/wiki/ACID#Isolation</link>
            <answer>Concurrent transaction execution results in the same state as if the transactions were executed serially.</answer>
        </question>

        <question id="acid-durability">
            <text>Durability</text>
            <link>https://en.wikipedia.org/wiki/ACID#Durability</link>
            <answer>Once a transaction is committed, the change of state will remain.</answer>
        </question>
    </section>

    <!-- This is not really a Google SRE theme: -->
    <section id="base" answers_as_choices="true">
        <title>BASE</title>
        <link>https://en.wikipedia.org/wiki/Eventual_consistency</link>
        <!-- http://www.dataversity.net/acid-vs-base-the-shifting-ph-of-database-transaction-processing/ -->

        <question id="base-basically-available">
            <text>Basically Available</text>
            <answer>Requests will be served, but could return outdated values.</answer>
        </question>

        <question id="base-soft-state">
            <text>Soft State</text>
            <answer>The system may change state even without external writes. For instance, to achieve eventual consistency.</answer>
        </question>

        <question id="base-eventual-consistency">
            <text>Eventual Consistency</text>
            <answer>Reads will all eventually return the latest value (converge), but consistency is not guaranteed at all times.</answer>
        </question>
    </section>

    <section id="projects" answers_as_choices="true">
        <title>Projects / Products</title>

        <question id="projects-cassandra">
            <text>Cassandra</text>
            <answer>Distributed Storage</answer>
            <link>https://en.wikipedia.org/wiki/Apache_Cassandra</link>
            <note>Shards via consistent hashing. No master (which would be a single point of failure).</note>
        </question>

        <question id="projects-redis">
            <text>Redis</text>
            <answer>Distributed Storage</answer>
            <link>https://en.wikipedia.org/wiki/Redis</link>
            <note>Uses Master/Slave. Doesn't seem to use consistent hashing for sharding, at least by default.</note>
        </question>

        <!-- TODO:
        <question id="projects-mongo-db">
            <text>MongoDB</text>
            <answer>Distributed Storage</answer>
            <link>https://en.wikipedia.org/wiki/MongoDB</link>
        </question>
        -->

        <question id="projects-hadoop">
            <text>Hadoop</text>
            <answer>Distributed Computation: Batch Processing (MapReduce)</answer>
            <link>https://en.wikipedia.org/wiki/Apache_Hadoop</link>
        </question>

        <question id="projects-spark">
            <text>Spark</text>
            <answer>Distributed Computation: Batch Processing</answer>
            <link>https://en.wikipedia.org/wiki/Apache_Spark</link>
            <video_url>https://youtu.be/DpqIc-EggOs?list=PLXCArKglxOG9pWzw23kPY-UcFMCuTLWFs</video_url>
            <note>Unlike Hadoop, Spark tries to keep data in memory, and doesn't restrict processing (transformation) to the MapReduce key/value API.</note>
        </question>

        <question id="projects-storm">
            <text>Storm</text>
            <answer>Distributed Computation: Stream Processing</answer>
            <link>https://en.wikipedia.org/wiki/Storm_(event_processor)</link>
        </question>

        <question id="projects-kafka">
            <text>Kafka</text>
            <answer>Distributed Message Queue</answer>
            <link>https://en.wikipedia.org/wiki/Apache_Kafka</link>
        </question>

        <question id="projects-zookeeper">
            <text>Zookeeper</text>
            <answer>Distributed Configuration and Coordination</answer>
            <link>https://en.wikipedia.org/wiki/Apache_ZooKeeper</link>
        </question>
    </section>

    <!--
    Availability

    Testing:
      Black box, white box.
      Load Testing
      Failure Modes
      Cascading Failures

    Monitoring: Black box, white box.

    Paxos, Kubernetes/Borg

    Subsetting
    Throttling
    Round Robin

    Cascading Failures:
     - Server Overload
     - Resource Exhaustion
     - Service Unavailability.

    Retries

    Distributed Consensus


    -->
</quiz>